from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File
from sqlalchemy.orm import Session
from typing import List, Optional
import logging

from backend.db.database import get_db
from backend.services.excel_batch_processor import ExcelBatchProcessor
from backend.schema.flight_data import (
    ExcelProcessRequest,
    ExcelProcessResponse,
    FlightRawResponse,
    MissingDimensionResponse,
    DataProcessingStats,
)
from backend.models.flight_raw import FlightRaw
from backend.models.missing_dimensions_log import MissingDimensionsLog


router = APIRouter(prefix="/data-processing", tags=["data-processing"])


@router.post("/process-excel", response_model=ExcelProcessResponse)
async def process_excel_data(
    request: ExcelProcessRequest, db: Session = Depends(get_db)
):
    """
    X·ª≠ l√Ω d·ªØ li·ªáu Excel v√† l∆∞u v√†o database - DEPRECATED
    S·ª≠ d·ª•ng /upload-files thay th·∫ø cho endpoint n√†y
    """
    try:
        processor = ExcelBatchProcessor(db)

        # T·∫°o temporary file t·ª´ request data ƒë·ªÉ process
        import tempfile
        import pandas as pd
        import os

        # Convert request data to DataFrame and save as temp Excel file
        df = pd.DataFrame(request.data)

        with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as temp_file:
            temp_path = temp_file.name
            df.to_excel(temp_path, index=False)

            try:
                # Process using ExcelBatchProcessor logic
                processed_df = processor.process_excel_file(temp_path, request.filename)

                if processed_df.empty:
                    return ExcelProcessResponse(
                        success=False,
                        message="Kh√¥ng th·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu Excel",
                        processed_count=0,
                        errors=["Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ x·ª≠ l√Ω"],
                    )

                # Save to database
                processor._save_to_database(processed_df)
                processed_count = len(processed_df)

                # Mark file as imported
                processor.mark_file_imported(request.filename, "excel", processed_count)

                # Run data cleaning stored procedure
                processor.run_data_cleaning_stored_procedure()

                db.commit()

                return ExcelProcessResponse(
                    success=True,
                    message=f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {processed_count} b·∫£n ghi t·ª´ file {request.filename}",
                    processed_count=processed_count,
                    duplicates_count=0,
                    errors=[],
                )

            finally:
                # Clean up temp file
                os.unlink(temp_path)

    except Exception as e:
        db.rollback()
        logging.error(f"Error processing Excel data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {str(e)}",
        )


@router.get("/flight-data", response_model=List[FlightRawResponse])
async def get_flight_data(
    skip: int = 0,
    limit: int = 100,
    route: Optional[str] = None,
    actype: Optional[str] = None,
    db: Session = Depends(get_db),
):
    """
    L·∫•y danh s√°ch d·ªØ li·ªáu flight raw
    """
    query = db.query(FlightRaw).order_by(FlightRaw.id.desc())

    if route:
        query = query.filter(FlightRaw.route.contains(route))

    if actype:
        query = query.filter(FlightRaw.actype.contains(actype))

    flights = query.offset(skip).limit(limit).all()
    return flights


@router.get("/missing-dimensions", response_model=List[MissingDimensionResponse])
async def get_missing_dimensions(
    dimension_type: Optional[str] = None, db: Session = Depends(get_db)
):
    """
    L·∫•y danh s√°ch missing dimensions
    """
    query = db.query(MissingDimensionsLog)

    if dimension_type:
        query = query.filter(MissingDimensionsLog.type == dimension_type)

    missing_dims = query.order_by(MissingDimensionsLog.count.desc()).all()
    return missing_dims


@router.get("/stats", response_model=DataProcessingStats)
async def get_processing_stats(db: Session = Depends(get_db)):
    """
    L·∫•y th·ªëng k√™ x·ª≠ l√Ω d·ªØ li·ªáu
    """
    try:
        processor = ExcelBatchProcessor(db)
        summary = processor.get_processing_summary()

        # Convert to DataProcessingStats format
        stats = {
            "total_flights": summary.get("raw_records", 0),
            "missing_actypes": summary.get("missing_actypes", 0),
            "missing_routes": summary.get("missing_routes", 0),
            "latest_import": None,  # Can add this to ExcelBatchProcessor if needed
            "file_count": summary.get("imported_files", 0),
        }

        return DataProcessingStats(**stats)

    except Exception as e:
        logging.error(f"Error getting processing stats: {e}")
        # Return empty stats if error
        return DataProcessingStats(
            total_flights=0,
            missing_actypes=0,
            missing_routes=0,
            latest_import=None,
            file_count=0,
        )


@router.post("/run-stored-procedure")
async def run_stored_procedure(db: Session = Depends(get_db)):
    """
    Ch·∫°y stored procedure ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu
    """
    try:
        processor = ExcelBatchProcessor(db)
        processor.run_data_cleaning_stored_procedure()

        return {"success": True, "message": "ƒê√£ ch·∫°y stored procedure th√†nh c√¥ng"}

    except Exception as e:
        logging.error(f"Error running stored procedure: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói ch·∫°y stored procedure: {str(e)}",
        )


@router.delete("/flight-data")
async def clear_flight_data(db: Session = Depends(get_db)):
    """
    X√≥a t·∫•t c·∫£ d·ªØ li·ªáu flight raw (ch·ªâ d√πng cho development)
    """
    try:
        # Delete all flight raw data
        db.query(FlightRaw).delete()

        # Delete import log
        from sqlalchemy import text

        db.execute(text("DELETE FROM import_log"))

        # Delete missing dimensions log
        db.query(MissingDimensionsLog).delete()

        db.commit()

        return {"success": True, "message": "ƒê√£ x√≥a t·∫•t c·∫£ d·ªØ li·ªáu"}

    except Exception as e:
        db.rollback()
        logging.error(f"Error clearing data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói x√≥a d·ªØ li·ªáu: {str(e)}",
        )


@router.post("/import-missing-dimensions")
async def import_missing_dimensions_data(db: Session = Depends(get_db)):
    """
    Ch·∫°y stored procedure ƒë·ªÉ import missing dimensions data t·ª´ temp tables
    Theo logic t·ª´ notebook ƒë·ªÉ x·ª≠ l√Ω aircraft types v√† routes thi·∫øu
    """
    try:
        from sqlalchemy import text

        print("üîß B·∫Øt ƒë·∫ßu import missing dimensions data...")

        processor = ExcelBatchProcessor(db)

        # Get missing dimensions before import
        before_summary = processor.get_processing_summary()
        print(
            f"üìä Tr∆∞·ªõc import: {before_summary['missing_actypes']} actypes thi·∫øu, {before_summary['missing_routes']} routes thi·∫øu"
        )

        # Run the stored procedure to import and update missing dimensions
        print("‚öôÔ∏è Ch·∫°y stored procedure: usp_ImportAndUpdateMissingDimensions")
        db.execute(text("EXEC usp_ImportAndUpdateMissingDimensions"))
        db.commit()

        # Get summary after import
        after_summary = processor.get_processing_summary()
        print(
            f"üìä Sau import: {after_summary['missing_actypes']} actypes thi·∫øu, {after_summary['missing_routes']} routes thi·∫øu"
        )

        # Calculate what was imported
        actypes_resolved = (
            before_summary["missing_actypes"] - after_summary["missing_actypes"]
        )
        routes_resolved = (
            before_summary["missing_routes"] - after_summary["missing_routes"]
        )

        return {
            "success": True,
            "message": f"ƒê√£ import missing dimensions th√†nh c√¥ng. Resolved {actypes_resolved} actypes v√† {routes_resolved} routes",
            "before_import": {
                "missing_actypes": before_summary["missing_actypes"],
                "missing_routes": before_summary["missing_routes"],
            },
            "after_import": {
                "missing_actypes": after_summary["missing_actypes"],
                "missing_routes": after_summary["missing_routes"],
            },
            "resolved": {"actypes": actypes_resolved, "routes": routes_resolved},
        }

    except Exception as e:
        db.rollback()
        error_msg = f"L·ªói import missing dimensions: {str(e)}"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=error_msg,
        )


@router.post("/batch-import-excel")
async def batch_import_excel_files(
    data_folder: str, destination_folder: str, db: Session = Depends(get_db)
):
    """
    Batch import t·∫•t c·∫£ file Excel t·ª´ folder theo logic notebook
    """
    try:
        processor = ExcelBatchProcessor(db)

        # Batch import files
        results = processor.batch_import_files(data_folder, destination_folder)

        if results["processed_files"] > 0:
            # Run data cleaning stored procedure
            processor.run_data_cleaning_stored_procedure()

        return {
            "success": True,
            "message": f"ƒê√£ x·ª≠ l√Ω {results['processed_files']} file v·ªõi t·ªïng {results['total_rows']} b·∫£n ghi",
            "processed_files": results["processed_files"],
            "total_rows": results["total_rows"],
            "skipped_files": results["skipped_files"],
            "errors": results["errors"],
            "file_details": results["file_details"],
        }

    except Exception as e:
        logging.error(f"Error in batch import: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói batch import: {str(e)}",
        )


@router.post("/run-data-cleaning")
async def run_data_cleaning(db: Session = Depends(get_db)):
    """
    Ch·∫°y stored procedure ƒë·ªÉ l√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu flight
    """
    try:
        processor = ExcelBatchProcessor(db)
        processor.run_data_cleaning_stored_procedure()

        return {"success": True, "message": "ƒê√£ ch·∫°y data cleaning th√†nh c√¥ng"}

    except Exception as e:
        logging.error(f"Error running data cleaning: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói ch·∫°y data cleaning: {str(e)}",
        )


@router.post("/revalidate-error-data")
async def revalidate_error_data(db: Session = Depends(get_db)):
    """
    Ch·∫°y stored procedure ƒë·ªÉ revalidate error data
    """
    try:
        from sqlalchemy import text

        db.execute(text("EXEC usp_RevalidateErrorData"))
        db.commit()

        return {"success": True, "message": "ƒê√£ revalidate error data th√†nh c√¥ng"}

    except Exception as e:
        db.rollback()
        logging.error(f"Error revalidating error data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói revalidate error data: {str(e)}",
        )


@router.get("/processing-summary")
async def get_processing_summary(db: Session = Depends(get_db)):
    """
    L·∫•y t√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu
    """
    try:
        processor = ExcelBatchProcessor(db)
        summary = processor.get_processing_summary()

        return {"success": True, "data": summary}

    except Exception as e:
        logging.error(f"Error getting processing summary: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"L·ªói l·∫•y processing summary: {str(e)}",
        )


@router.post("/export-missing-dimensions")
async def export_missing_dimensions_to_excel(db: Session = Depends(get_db)):
    """
    Xu·∫•t missing dimensions ra file Excel theo logic notebook
    T·∫°o file Add_information.xlsx v·ªõi 3 sheets: Actype_seat, Route, Airline_Route_Details
    """
    try:
        import pandas as pd
        import tempfile
        import os
        from fastapi.responses import FileResponse
        from sqlalchemy import text

        print("üì§ B·∫Øt ƒë·∫ßu export missing dimensions...")

        # Query 1: Actype_seat - missing actypes from error table
        query1 = """
        SELECT TOP 0 actype, seat
        FROM actype_seat
        UNION ALL
        SELECT Value as actype, NULL as seat
        FROM Missing_Dimensions_Log
        WHERE Type = 'ACTYPE'
        GROUP BY Value
        """

        # Query 2: Route - missing routes from error table
        query2 = """
        SELECT TOP 0
            [ROUTE],
            [AC],
            [Route_ID],
            [FLIGHT HOUR],
            [TAXI],
            [BLOCK HOUR],
            [DISTANCE KM],
            [Lo·∫°i],
            [Type],
            [Country]
        FROM Route
        UNION
        SELECT Value as [ROUTE],
            NULL AS [AC],
            NULL AS [Route_ID],
            NULL AS [FLIGHT HOUR],
            NULL AS [TAXI],
            NULL AS [BLOCK HOUR],
            NULL AS [DISTANCE KM],
            NULL AS [Lo·∫°i],
            NULL AS [Type],
            NULL AS [Country]
        FROM Missing_Dimensions_Log
        WHERE Type = 'ROUTE'
        AND Value IS NOT NULL
        """

        # Query 3: Airline_Route_Details - missing routes for route details
        query3 = """
        SELECT TOP 0
            [Sector] as [ROUTE],
            [Distance mile GDS],
            [Distance km GDS],
            [Sector_2],
            [Country 1],
            [Country 2],
            [Country],
            [DOM/INT],
            [Area]
        FROM Airline_Route_Details
        UNION
        SELECT Value as [ROUTE],
            NULL as [Distance mile GDS],
            NULL as [Distance km GDS],
            NULL as [Sector_2],
            NULL as [Country 1],
            NULL as [Country 2],
            NULL as [Country],
            NULL as [DOM/INT],
            NULL as [Area]
        FROM Missing_Dimensions_Log
        WHERE Type = 'ROUTE'
        AND Value IS NOT NULL
        """

        # Execute queries and create DataFrames
        df1 = pd.read_sql(query1, db.bind)
        df2 = pd.read_sql(query2, db.bind)
        df3 = pd.read_sql(query3, db.bind)

        print(
            f"üìä Export data: {len(df1)} actypes, {len(df2)} routes, {len(df3)} route details"
        )

        # Create temporary Excel file
        with tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False) as temp_file:
            temp_path = temp_file.name

            # Write to Excel with multiple sheets
            with pd.ExcelWriter(temp_path, engine="openpyxl") as writer:
                df1.to_excel(writer, sheet_name="Actype_seat", index=False)
                df2.to_excel(writer, sheet_name="Route", index=False)
                df3.to_excel(writer, sheet_name="Airline_Route_Details", index=False)

        print(f"‚úÖ ƒê√£ t·∫°o file Excel: Add_information.xlsx")

        # Return file response
        return FileResponse(
            path=temp_path,
            filename="Add_information.xlsx",
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )

    except Exception as e:
        error_msg = f"L·ªói export missing dimensions: {str(e)}"
        print(f"‚ùå {error_msg}")
        logging.error(error_msg)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_msg
        )


@router.post("/upload-files")
async def upload_excel_files(
    files: List[UploadFile] = File(...), db: Session = Depends(get_db)
):
    """
    Upload v√† x·ª≠ l√Ω multiple Excel files
    - Import d·ªØ li·ªáu t·ª´ file Excel
    - Ph√¢n lo·∫°i file theo MN, MB, MT
    - X·ª≠ l√Ω data theo t·ª´ng lo·∫°i
    - Ch·∫°y stored procedures ƒë·ªÉ clean v√† enrich data

    Args:
        files (List[UploadFile]): Danh s√°ch file Excel
        db (Session): Session c·ªßa database

    Returns:
        Dict[str, Any]: K·∫øt qu·∫£ x·ª≠ l√Ω
    """

    try:
        import tempfile
        import os

        # All necessary functions
        processor = ExcelBatchProcessor(db)

        results = {
            "processed_files": 0,
            "total_rows": 0,
            "skipped_files": 0,
            "errors": [],
            "file_details": [],
            "processing_summary": {},
        }

        print(f"üì§ B·∫Øt ƒë·∫ßu upload v√† x·ª≠ l√Ω {len(files)} file Excel...")

        # Create temp directory for processing
        with tempfile.TemporaryDirectory() as temp_dir:
            # Save uploaded files to temp directory
            for file in files:
                if not file.filename.lower().endswith((".xlsx", ".xls")):
                    results["errors"].append(
                        f"File {file.filename} kh√¥ng ph·∫£i l√† Excel file"
                    )
                    continue

                # Extract only filename from path (in case of folder upload)
                filename_only = os.path.basename(file.filename)
                file_path = os.path.join(temp_dir, filename_only)

                # Save file
                with open(file_path, "wb") as buffer:
                    content = await file.read()
                    buffer.write(content)

                try:
                    # Console log filename for debugging folder uploads
                    print(f"üìÅ Processing file: {file.filename} -> {filename_only}")

                    # Check if file already imported
                    if processor.is_file_imported(filename_only):
                        print(f"‚è≠Ô∏è File {filename_only} ƒë√£ ƒë∆∞·ª£c import tr∆∞·ªõc ƒë√≥")
                        results["skipped_files"] += 1
                        continue

                    print(f"üîÑ ƒêang x·ª≠ l√Ω file: {filename_only}")

                    # Determine file type first
                    file_type = processor.find_matching_key(filename_only)
                    if not file_type:
                        results["errors"].append(
                            f"Kh√¥ng th·ªÉ x√°c ƒë·ªãnh lo·∫°i file: {filename_only}"
                        )
                        continue

                    print(f"üìÅ Lo·∫°i file: {file_type} - {filename_only}")

                    # Process Excel file using notebook logic
                    df = processor.process_excel_file(file_path, filename_only)

                    if df.empty:
                        results["errors"].append(
                            f"Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ file {filename_only}"
                        )
                        continue

                    row_count = len(df)
                    print(f"üìä Extracted {row_count} rows t·ª´ {filename_only}")

                    # Save to flight_raw table
                    processor._save_to_database(df)

                    # Mark file as imported with file type
                    processor.mark_file_imported(filename_only, file_type, row_count)

                    results["processed_files"] += 1
                    results["total_rows"] += row_count
                    results["file_details"].append(
                        {
                            "file_name": filename_only,
                            "file_type": file_type,
                            "rows": row_count,
                        }
                    )

                    print(f"‚úÖ ƒê√£ l∆∞u {row_count} b·∫£n ghi t·ª´ file {filename_only}")

                except Exception as e:
                    error_msg = f"L·ªói x·ª≠ l√Ω file {filename_only}: {str(e)}"
                    print(f"‚ùå {error_msg}")
                    results["errors"].append(error_msg)

            # Commit raw data first
            db.commit()
            print(f"üíæ ƒê√£ commit {results['total_rows']} b·∫£n ghi raw data")

            # Run data cleaning and processing if files were processed
            if results["processed_files"] > 0:
                print("üßπ B·∫Øt ƒë·∫ßu qu√° tr√¨nh l√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu...")

                try:
                    # Step 1: Clean and process flight data
                    print("1Ô∏è‚É£ Ch·∫°y stored procedure: usp_CleanAndProcessFlightData")
                    processor.run_data_cleaning_stored_procedure()

                    # Step 2: Validate and move error data
                    print("2Ô∏è‚É£ Ch·∫°y stored procedure: usp_CleanAndValidateFlightData")
                    from sqlalchemy import text

                    db.execute(text("EXEC usp_CleanAndValidateFlightData"))
                    db.commit()

                    # Get processing summary
                    results["processing_summary"] = processor.get_processing_summary()

                    print("‚úÖ Ho√†n th√†nh qu√° tr√¨nh l√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu")

                except Exception as sp_error:
                    print(f"‚ö†Ô∏è L·ªói khi ch·∫°y stored procedures: {sp_error}")
                    results["errors"].append(f"L·ªói stored procedure: {str(sp_error)}")

        success_message = f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {results['processed_files']} file v·ªõi t·ªïng {results['total_rows']} b·∫£n ghi"
        if results["skipped_files"] > 0:
            success_message += f" (b·ªè qua {results['skipped_files']} file ƒë√£ import)"

        print(f"üéâ {success_message}")

        return {"success": True, "message": success_message, **results}

    except Exception as e:
        db.rollback()
        error_msg = f"L·ªói upload files: {str(e)}"
        print(f"üí• {error_msg}")
        logging.error(error_msg)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_msg
        )


@router.post("/complete-workflow")
async def complete_data_processing_workflow(
    files: List[UploadFile] = File(...), db: Session = Depends(get_db)
):
    """
    Workflow ho√†n ch·ªânh theo logic notebook:
    1. Upload v√† x·ª≠ l√Ω Excel files (MN, MB, MT)
    2. Import d·ªØ li·ªáu raw v√†o flight_raw
    3. Ch·∫°y stored procedures ƒë·ªÉ clean v√† validate
    4. Export missing dimensions
    5. Tr·∫£ v·ªÅ summary v√† file Excel cho missing data
    """
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu complete workflow x·ª≠ l√Ω d·ªØ li·ªáu Excel...")

        # Step 1: Upload and process Excel files
        upload_result = await upload_excel_files(files, db)

        if not upload_result["success"]:
            return upload_result

        print(f"‚úÖ Step 1 ho√†n th√†nh: Upload {upload_result['processed_files']} files")

        # Step 2: Get processing summary after initial processing
        processor = ExcelBatchProcessor(db)
        summary_after_processing = processor.get_processing_summary()

        print(f"üìä Step 2: Summary sau x·ª≠ l√Ω - {summary_after_processing}")

        # Step 3: If there are missing dimensions, prepare export data
        missing_data_info = None
        if (
            summary_after_processing["missing_actypes"] > 0
            or summary_after_processing["missing_routes"] > 0
        ):
            print(
                f"‚ö†Ô∏è Ph√°t hi·ªán missing dimensions: {summary_after_processing['missing_actypes']} actypes, {summary_after_processing['missing_routes']} routes"
            )

            # Create export data info (without actually creating file here)
            missing_data_info = {
                "missing_actypes": summary_after_processing["missing_actypes"],
                "missing_routes": summary_after_processing["missing_routes"],
                "export_available": True,
                "message": "C√≥ d·ªØ li·ªáu thi·∫øu c·∫ßn b·ªï sung. S·ª≠ d·ª•ng endpoint /export-missing-dimensions ƒë·ªÉ t·∫£i file Excel.",
            }

        # Final summary
        final_summary = {
            "workflow_completed": True,
            "files_processed": upload_result["processed_files"],
            "total_rows": upload_result["total_rows"],
            "processing_summary": summary_after_processing,
            "missing_data_info": missing_data_info,
            "next_steps": [],
        }

        # Add next steps recommendations
        if missing_data_info:
            final_summary["next_steps"].append(
                "1. T·∫£i file missing dimensions b·∫±ng endpoint /export-missing-dimensions"
            )
            final_summary["next_steps"].append("2. ƒêi·ªÅn th√¥ng tin thi·∫øu v√†o file Excel")
            final_summary["next_steps"].append("3. Upload file ƒë√£ ƒëi·ªÅn v√†o temp tables")
            final_summary["next_steps"].append(
                "4. Ch·∫°y endpoint /import-missing-dimensions ƒë·ªÉ c·∫≠p nh·∫≠t"
            )
            final_summary["next_steps"].append(
                "5. Ch·∫°y l·∫°i /revalidate-error-data ƒë·ªÉ x·ª≠ l√Ω l·∫°i errors"
            )
        else:
            final_summary["next_steps"].append(
                "‚úÖ Kh√¥ng c√≥ d·ªØ li·ªáu thi·∫øu, workflow ho√†n t·∫•t!"
            )

        print("üéâ Complete workflow ho√†n th√†nh!")

        return {
            "success": True,
            "message": "Complete workflow ƒë√£ ho√†n th√†nh th√†nh c√¥ng",
            "data": final_summary,
        }

    except Exception as e:
        error_msg = f"L·ªói complete workflow: {str(e)}"
        print(f"üí• {error_msg}")
        logging.error(error_msg)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_msg
        )
