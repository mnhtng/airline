# üìä PH√ÇN T√çCH LOGIC X·ª¨ L√ù BATCH FILE EXCEL

## T·ªïng quan

Document n√†y ph√¢n t√≠ch chi ti·∫øt lu·ªìng x·ª≠ l√Ω batch file Excel t·ª´ Frontend (React) ƒë·∫øn Backend (FastAPI), bao g·ªìm c√°c b∆∞·ªõc import d·ªØ li·ªáu, x·ª≠ l√Ω, l√†m s·∫°ch v√† validate.

---

## üéØ FLOW T·ªîNG QUAN

```mermaid
graph TD
    A[User Upload Files] --> B[Frontend: T·∫°o FormData]
    B --> C[POST /upload-files]
    C --> D[Backend: L∆∞u v√†o Temp Directory]
    D --> E[Loop qua t·ª´ng file]
    E --> F{File ƒë√£ import?}
    F -->|Yes| G[Skip file, tƒÉng counter]
    F -->|No| H[X√°c ƒë·ªãnh lo·∫°i file]
    H --> I[ƒê·ªçc & x·ª≠ l√Ω Excel]
    I --> J[L∆∞u v√†o flight_raw]
    J --> K[Ghi log import]
    K --> L[Commit raw data]
    L --> M[Run Stored Procedures]
    M --> N[Get Processing Summary]
    N --> O[Return Results]
    O --> P[Frontend: Hi·ªÉn th·ªã k·∫øt qu·∫£]
```

---

## üì± FRONTEND LOGIC - Index.tsx

### B∆∞·ªõc 1: Ng∆∞·ªùi d√πng upload files

**Location:** `processBatchExcelData()` function

```typescript
const processBatchExcelData = async (files: ExcelFile[]) => {
    setIsProcessing(true)

    try {
        // Prepare FormData with original files
        const formData = new FormData()
        files.forEach((fileData: ExcelFile) => {
            formData.append('files', fileData.file)
        })

        const response = await fetch(`${import.meta.env.VITE_API_URL}/data-processing/upload-files`, {
            method: 'POST',
            body: formData
        })
    }
}
```

**Chi ti·∫øt th·ª±c hi·ªán:**

1. **Set loading state**: `setIsProcessing(true)`
2. **T·∫°o FormData object**: Container ƒë·ªÉ g·ª≠i multipart/form-data
3. **Append files**: Loop qua array `files`, append t·ª´ng `file.file` (File object g·ªëc)
4. **Send POST request**:
   - Endpoint: `/data-processing/upload-files`
   - Method: `POST`
   - Body: `formData` (ch·ª©a multiple files)
   - Header: T·ª± ƒë·ªông set `Content-Type: multipart/form-data`

---

### B∆∞·ªõc 2: X·ª≠ l√Ω Response t·ª´ API

```typescript
if (!response.ok) {
    const errorText = await response.text()
    console.error('API Error Response:', errorText)

    toast.error("L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu", {
        description: "Vui l√≤ng th·ª≠ l·∫°i.",
    })
    return
}

const result = await response.json()

// Map all response fields to ProcessResult
setProcessResult({
    success: result.success,
    message: result.message,
    processed_count: result.total_rows || 0,
    processed_files: result.processed_files,
    total_rows: result.total_rows,
    skipped_files: result.skipped_files,
    errors: result.errors,
    file_details: result.file_details,
    processing_summary: result.processing_summary,
})
```

**Chi ti·∫øt x·ª≠ l√Ω:**

1. **Ki·ªÉm tra response status**:
   - N·∫øu `!response.ok` (status code 4xx/5xx)
   - Log error v√† hi·ªÉn th·ªã toast error
   - Return s·ªõm, kh√¥ng x·ª≠ l√Ω ti·∫øp

2. **Parse JSON response**: `await response.json()`

3. **Map response v√†o state**:
   - `success`: Tr·∫°ng th√°i x·ª≠ l√Ω
   - `message`: Th√¥ng b√°o t·ªïng quan
   - `processed_count`: T·ªïng s·ªë b·∫£n ghi ƒë√£ x·ª≠ l√Ω
   - `processed_files`: S·ªë file ƒë√£ x·ª≠ l√Ω
   - `total_rows`: T·ªïng s·ªë d√≤ng d·ªØ li·ªáu
   - `skipped_files`: S·ªë file b·ªã skip (ƒë√£ import tr∆∞·ªõc ƒë√≥)
   - `errors`: Array ch·ª©a c√°c l·ªói
   - `file_details`: Chi ti·∫øt t·ª´ng file ƒë√£ x·ª≠ l√Ω
   - `processing_summary`: Th·ªëng k√™ chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω

---

### B∆∞·ªõc 3: Hi·ªÉn th·ªã Notification

#### 3.1. Success Notification

```typescript
if (result.success) {
    // Build detailed success message
    let description = result.message

    if (result.processed_files && result.total_rows) {
        description += `\nüìä ƒê√£ x·ª≠ l√Ω: ${result.processed_files} file v·ªõi ${result.total_rows} b·∫£n ghi`
    }

    if (result.skipped_files && result.skipped_files > 0) {
        description += `\n‚è≠Ô∏è ƒê√£ b·ªè qua: ${result.skipped_files} file ƒë√£ import tr∆∞·ªõc ƒë√≥`
    }

    if (result.processing_summary) {
        const summary = result.processing_summary
        description += `\n‚úÖ Processed: ${summary.processed_records} records`
        if (summary.error_records > 0) {
            description += `\n‚ùå Errors: ${summary.error_records} records`
        }
        if (summary.missing_actypes > 0 || summary.missing_routes > 0) {
            description += `\n‚ö†Ô∏è Missing: ${summary.missing_actypes} actypes, ${summary.missing_routes} routes`
        }
    }

    toast.success("X·ª≠ l√Ω d·ªØ li·ªáu th√†nh c√¥ng", {
        description: description,
    })
}
```

**C·∫•u tr√∫c message:**

- Message ch√≠nh t·ª´ API
- üìä Th√¥ng tin file ƒë√£ x·ª≠ l√Ω
- ‚è≠Ô∏è File b·ªã skip (n·∫øu c√≥)
- ‚úÖ S·ªë b·∫£n ghi processed
- ‚ùå S·ªë b·∫£n ghi l·ªói (n·∫øu c√≥)
- ‚ö†Ô∏è Missing dimensions (n·∫øu c√≥)

#### 3.2. Error Notifications

```typescript
// Handle errors array if present
if (result.errors && result.errors.length > 0) {
    console.warn('Processing Errors:', result.errors)

    // Show first few errors as separate notifications
    result.errors.slice(0, 3).forEach((error: string, index: number) => {
        setTimeout(() => {
            toast.error(`L·ªói ${index + 1}`, {
                description: error,
            })
        }, (index + 1) * 1000)
    })

    if (result.errors.length > 3) {
        setTimeout(() => {
            toast.info("C√≥ th√™m l·ªói kh√°c", {
                description: `V√† ${result.errors.length - 3} l·ªói kh√°c. Ki·ªÉm tra console ƒë·ªÉ xem chi ti·∫øt.`,
            })
        }, 4000)
    }
}
```

**Logic hi·ªÉn th·ªã l·ªói:**

- Log t·∫•t c·∫£ errors v√†o console
- Hi·ªÉn th·ªã t·ªëi ƒëa 3 l·ªói ƒë·∫ßu ti√™n
- M·ªói l·ªói delay 1 gi√¢y ƒë·ªÉ user ƒë·ªçc ƒë∆∞·ª£c
- N·∫øu > 3 l·ªói: hi·ªÉn th·ªã th√¥ng b√°o "c√≥ th√™m l·ªói kh√°c"

---

### B∆∞·ªõc 4: Hi·ªÉn th·ªã Processing Summary UI

```typescript
{processResult && (
    <div className={`p-6 rounded-lg space-y-4 ${processResult.success ? 'bg-green-50 border border-green-200' : 'bg-red-50 border border-red-200'}`}>
        {/* Header */}
        <div className="flex items-start space-x-3">
            {processResult.success ? (
                <CheckCircle className="h-6 w-6 text-green-600 mt-0.5 flex-shrink-0" />
            ) : (
                <AlertCircle className="h-6 w-6 text-red-600 mt-0.5 flex-shrink-0" />
            )}
            <div className="flex-1">
                <h4 className={`font-semibold text-lg ${processResult.success ? 'text-green-800' : 'text-red-800'}`}>
                    K·∫øt qu·∫£ x·ª≠ l√Ω d·ªØ li·ªáu
                </h4>
                <p className={`text-sm ${processResult.success ? 'text-green-700' : 'text-red-700'}`}>
                    {processResult.message}
                </p>
            </div>
        </div>

        {/* Processing Summary Statistics */}
        {processResult.processing_summary && (
            <div className="grid grid-cols-2 md:grid-cols-3 lg:grid-cols-6 gap-4 mt-4">
                <StatCard value={processResult.processing_summary.raw_records} label="B·∫£n ghi g·ªëc" color="blue" />
                <StatCard value={processResult.processing_summary.processed_records} label="B·∫£n ghi ƒë√£ x·ª≠ l√Ω" color="green" />
                <StatCard value={processResult.processing_summary.error_records} label="L·ªói" color="red" />
                <StatCard value={processResult.processing_summary.missing_actypes} label="Actypes thi·∫øu" color="orange" />
                <StatCard value={processResult.processing_summary.missing_routes} label="Routes thi·∫øu" color="orange" />
                <StatCard value={processResult.processing_summary.imported_files} label="Files ƒë√£ import" color="gray" />
            </div>
        )}
    </div>
)}
```

**C√°c th√†nh ph·∫ßn UI:**

1. **Header**: Icon + title + message (m√†u theo success/fail)
2. **Statistics Grid**: 6 cards hi·ªÉn th·ªã metrics
3. **File Details**: List c√°c file ƒë√£ x·ª≠ l√Ω
4. **Errors Display**: Hi·ªÉn th·ªã chi ti·∫øt l·ªói (n·∫øu c√≥)

---

## üîß BACKEND LOGIC - data_processing.py

### B∆∞·ªõc 1: Kh·ªüi t·∫°o v√† Validate

```python
@router.post("/upload-files")
async def upload_excel_files(
    files: List[UploadFile] = File(...), db: Session = Depends(get_db)
):
    """
    Upload v√† x·ª≠ l√Ω multiple Excel files
    - Import d·ªØ li·ªáu t·ª´ file Excel
    - Ph√¢n lo·∫°i file theo MN, MB, MT
    - X·ª≠ l√Ω data theo t·ª´ng lo·∫°i
    - Ch·∫°y stored procedures ƒë·ªÉ clean v√† enrich data
    """

    try:
        import tempfile
        import os

        # Initialize processor
        processor = ExcelBatchProcessor(db)

        # Initialize results tracking
        results = {
            "processed_files": 0,
            "total_rows": 0,
            "skipped_files": 0,
            "errors": [],
            "file_details": [],
            "processing_summary": {},
        }

        print(f"üì§ B·∫Øt ƒë·∫ßu upload v√† x·ª≠ l√Ω {len(files)} file Excel...")
```

**Chi ti·∫øt:**

1. **Dependencies**:
   - `files`: List[UploadFile] - Array c·ªßa FastAPI UploadFile objects
   - `db`: Session - SQLAlchemy database session

2. **Kh·ªüi t·∫°o processor**:
   - `ExcelBatchProcessor(db)` - Class ch·ª©a logic x·ª≠ l√Ω Excel

3. **Results dictionary** tracking:
   - `processed_files`: Counter s·ªë file th√†nh c√¥ng
   - `total_rows`: T·ªïng s·ªë d√≤ng d·ªØ li·ªáu
   - `skipped_files`: S·ªë file b·ªã skip
   - `errors`: Array c√°c error messages
   - `file_details`: Chi ti·∫øt t·ª´ng file
   - `processing_summary`: Th·ªëng k√™ sau khi x·ª≠ l√Ω

---

### B∆∞·ªõc 2: L∆∞u Files v√†o Temp Directory

```python
# Create temp directory for processing
with tempfile.TemporaryDirectory() as temp_dir:
    # Save uploaded files to temp directory
    for file in files:
        if not file.filename.lower().endswith((".xlsx", ".xls")):
            results["errors"].append(
                f"File {file.filename} kh√¥ng ph·∫£i l√† Excel file"
            )
            continue

        # Extract only filename from path (in case of folder upload)
        filename_only = os.path.basename(file.filename)
        file_path = os.path.join(temp_dir, filename_only)

        # Save file
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
```

**Chi ti·∫øt t·ª´ng b∆∞·ªõc:**

1. **T·∫°o Temp Directory**:
   - `tempfile.TemporaryDirectory()` - Context manager
   - T·ª± ƒë·ªông cleanup khi exit context
   - OS s·∫Ω x√≥a directory v√† t·∫•t c·∫£ files b√™n trong

2. **Loop qua t·ª´ng file**:

   **2.1. Validate Extension**
   - Check extension: `.xlsx` ho·∫∑c `.xls`
   - N·∫øu invalid: th√™m v√†o `errors[]`, continue

   **2.2. Extract Filename**
   - `os.path.basename(file.filename)`
   - Lo·∫°i b·ªè path (quan tr·ªçng cho folder upload)
   - VD: `folder1/folder2/file.xlsx` ‚Üí `file.xlsx`

   **2.3. Construct File Path**
   - `os.path.join(temp_dir, filename_only)`
   - T·∫°o full path trong temp directory

   **2.4. Save File Content**
   - `await file.read()` - Async read file content
   - Write binary content v√†o file path

---

### B∆∞·ªõc 3: X·ª≠ l√Ω t·ª´ng File

```python
try:
    # Console log filename for debugging folder uploads
    print(f"üìÅ Processing file: {file.filename} -> {filename_only}")

    # Check if file already imported
    if processor.is_file_imported(filename_only):
        print(f"‚è≠Ô∏è File {filename_only} ƒë√£ ƒë∆∞·ª£c import tr∆∞·ªõc ƒë√≥")
        results["skipped_files"] += 1
        continue

    print(f"üîÑ ƒêang x·ª≠ l√Ω file: {filename_only}")

    # Determine file type first
    file_type = processor.find_matching_key(filename_only)
    if not file_type:
        results["errors"].append(
            f"Kh√¥ng th·ªÉ x√°c ƒë·ªãnh lo·∫°i file: {filename_only}"
        )
        continue

    print(f"üìÅ Lo·∫°i file: {file_type} - {filename_only}")

    # Process Excel file using notebook logic
    df = processor.process_excel_file(file_path, filename_only)

    if df.empty:
        results["errors"].append(
            f"Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ file {filename_only}"
        )
        continue

    row_count = len(df)
    print(f"üìä Extracted {row_count} rows t·ª´ {filename_only}")

    # Save to flight_raw table
    processor._save_to_database(df)

    # Mark file as imported with file type
    processor.mark_file_imported(filename_only, file_type, row_count)

    results["processed_files"] += 1
    results["total_rows"] += row_count
    results["file_details"].append(
        {
            "file_name": filename_only,
            "file_type": file_type,
            "rows": row_count,
        }
    )

    print(f"‚úÖ ƒê√£ l∆∞u {row_count} b·∫£n ghi t·ª´ file {filename_only}")

except Exception as e:
    error_msg = f"L·ªói x·ª≠ l√Ω file {filename_only}: {str(e)}"
    print(f"‚ùå {error_msg}")
    results["errors"].append(error_msg)
```

#### 3.1. Ki·ªÉm tra File ƒë√£ Import ch∆∞a

**Method:** `processor.is_file_imported(filename_only)`

```python
def is_file_imported(self, filename: str) -> bool:
    """Ki·ªÉm tra xem file ƒë√£ ƒë∆∞·ª£c import ch∆∞a"""
    from backend.models.import_log import ImportLog
    
    existing = self.db.query(ImportLog).filter(
        ImportLog.file_name == filename
    ).first()
    
    return existing is not None
```

**Logic:**

- Query b·∫£ng `import_log`
- Filter theo `file_name`
- Tr·∫£ v·ªÅ `True` n·∫øu t·ªìn t·∫°i, `False` n·∫øu ch∆∞a c√≥
- **Purpose**: Tr√°nh import duplicate file

**N·∫øu ƒë√£ import:**

- Log message skip
- TƒÉng counter `skipped_files`
- `continue` (skip file n√†y)

---

#### 3.2. X√°c ƒë·ªãnh Lo·∫°i File

**Method:** `processor.find_matching_key(filename_only)`

```python
def find_matching_key(self, filename: str) -> Optional[str]:
    """
    X√°c ƒë·ªãnh lo·∫°i file d·ª±a v√†o t√™n
    Returns: 'MN' | 'MB' | 'MT' | None
    """
    filename_lower = filename.lower()
    
    # Mapping patterns
    patterns = {
        'MN': ['mn', 'mi·ªÅn nam', 'mien nam'],
        'MB': ['mb', 'mi·ªÅn b·∫Øc', 'mien bac'],
        'MT': ['mt', 'mi·ªÅn trung', 'mien trung']
    }
    
    for key, keywords in patterns.items():
        if any(keyword in filename_lower for keyword in keywords):
            return key
    
    return None
```

**Logic:**

- Convert filename to lowercase
- Check v·ªõi c√°c patterns:
    - **MN**: Mi·ªÅn Nam - `['mn', 'mi·ªÅn nam', 'mien nam']`
    - **MB**: Mi·ªÅn B·∫Øc - `['mb', 'mi·ªÅn b·∫Øc', 'mien bac']`
    - **MT**: Mi·ªÅn Trung - `['mt', 'mi·ªÅn trung', 'mien trung']`
- Tr·∫£ v·ªÅ key t∆∞∆°ng ·ª©ng ho·∫∑c `None`

**N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c:**

- Th√™m error message v√†o `errors[]`
- `continue` (skip file n√†y)

---

#### 3.3. X·ª≠ l√Ω Excel File

**Method:** `processor.process_excel_file(file_path, filename_only)`

```python
def process_excel_file(self, file_path: str, filename: str) -> pd.DataFrame:
    """
    X·ª≠ l√Ω Excel file:
    1. ƒê·ªçc Excel v·ªõi pandas
    2. Map columns theo file type
    3. Clean v√† transform data
    4. Return DataFrame
    """
    
    # 1. Read Excel
    df = pd.read_excel(file_path)
    
    # 2. Determine file type and get column mapping
    file_type = self.find_matching_key(filename)
    column_mapping = self.get_column_mapping(file_type)
    
    # 3. Rename columns
    df = df.rename(columns=column_mapping)
    
    # 4. Clean data
    df = self.clean_dataframe(df)
    
    # 5. Add metadata
    df['source_file'] = filename
    df['import_date'] = datetime.now()
    
    return df
```

**Chi ti·∫øt c√°c b∆∞·ªõc:**

**Step 1: ƒê·ªçc Excel**

- `pd.read_excel(file_path)` - ƒê·ªçc file Excel
- T·ª± ƒë·ªông detect sheets, headers

**Step 2: Get Column Mapping**

- Mapping columns theo file type (MN/MB/MT)
- M·ªói lo·∫°i file c√≥ c·∫•u tr√∫c columns kh√°c nhau

**Step 3: Rename Columns**

- Chu·∫©n h√≥a t√™n columns
- Map sang schema c·ªßa database

**Step 4: Clean Data**

- Remove duplicates
- Handle missing values
- Format dates, numbers
- Trim whitespaces

**Step 5: Add Metadata**

- `source_file`: T√™n file g·ªëc
- `import_date`: Timestamp import

---

#### 3.4. Validate DataFrame

```python
if df.empty:
    results["errors"].append(
        f"Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ file {filename_only}"
    )
    continue

row_count = len(df)
print(f"üìä Extracted {row_count} rows t·ª´ {filename_only}")
```

**Validation:**

- Check `df.empty` (DataFrame r·ªóng)
- N·∫øu r·ªóng: log error, skip file
- N·∫øu c√≥ data: t√≠nh s·ªë rows

---

#### 3.5. L∆∞u v√†o Database

**Method:** `processor._save_to_database(df)`

```python
def _save_to_database(self, df: pd.DataFrame):
    """
    L∆∞u DataFrame v√†o b·∫£ng flight_raw
    """
    from backend.models.flight_raw import FlightRaw
    
    # Convert DataFrame to list of dicts
    records = df.to_dict('records')
    
    # Bulk insert
    for record in records:
        flight = FlightRaw(**record)
        self.db.add(flight)
    
    # Note: Kh√¥ng commit ·ªü ƒë√¢y, s·∫Ω commit sau khi process t·∫•t c·∫£ files
```

**Logic:**

- Convert DataFrame ‚Üí list of dictionaries
- Loop qua t·ª´ng record
- Create `FlightRaw` model instance
- Add v√†o session
- **Ch∆∞a commit** - s·∫Ω commit sau

---

#### 3.6. Ghi Log Import

**Method:** `processor.mark_file_imported(filename, file_type, row_count)`

```python
def mark_file_imported(self, filename: str, file_type: str, row_count: int):
    """
    Ghi log file ƒë√£ import v√†o import_log table
    """
    from backend.models.import_log import ImportLog
    
    import_log = ImportLog(
        file_name=filename,
        file_type=file_type,
        row_count=row_count,
        import_date=datetime.now(),
        status='success'
    )
    
    self.db.add(import_log)
```

**Purpose:**

- Track files ƒë√£ import
- Tr√°nh duplicate import
- Audit trail

**Fields:**

- `file_name`: T√™n file
- `file_type`: MN/MB/MT
- `row_count`: S·ªë d√≤ng ƒë√£ import
- `import_date`: Timestamp
- `status`: success/failed

---

#### 3.7. C·∫≠p nh·∫≠t Results

```python
results["processed_files"] += 1
results["total_rows"] += row_count
results["file_details"].append(
    {
        "file_name": filename_only,
        "file_type": file_type,
        "rows": row_count,
    }
)

print(f"‚úÖ ƒê√£ l∆∞u {row_count} b·∫£n ghi t·ª´ file {filename_only}")
```

**Updates:**

- Increment `processed_files` counter
- Add `row_count` v√†o `total_rows`
- Append file detail object v√†o array
- Log success message

---

### B∆∞·ªõc 4: Commit Raw Data

```python
# Commit raw data first
db.commit()
print(f"üíæ ƒê√£ commit {results['total_rows']} b·∫£n ghi raw data")
```

**Chi ti·∫øt:**

- Commit t·∫•t c·∫£ raw data v√†o database
- ƒê·∫£m b·∫£o data ƒë∆∞·ª£c persist tr∆∞·ªõc khi x·ª≠ l√Ω ti·∫øp
- Transaction boundary: All-or-nothing cho raw data

**T·∫°i sao commit ·ªü ƒë√¢y?**

1. Raw data ƒë√£ ho√†n ch·ªânh
2. T√°ch bi·ªát transaction raw import vs data cleaning
3. N·∫øu cleaning fail, raw data v·∫´n ƒë∆∞·ª£c gi·ªØ l·∫°i
4. C√≥ th·ªÉ rerun cleaning m√† kh√¥ng c·∫ßn re-import

---

### B∆∞·ªõc 5: Ch·∫°y Stored Procedures

```python
# Run data cleaning and processing if files were processed
if results["processed_files"] > 0:
    print("üßπ B·∫Øt ƒë·∫ßu qu√° tr√¨nh l√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu...")

    try:
        # Step 1: Clean and process flight data
        print("1Ô∏è‚É£ Ch·∫°y stored procedure: usp_CleanAndProcessFlightData")
        processor.run_data_cleaning_stored_procedure()

        # Step 2: Validate and move error data
        print("2Ô∏è‚É£ Ch·∫°y stored procedure: usp_CleanAndValidateFlightData")
        from sqlalchemy import text

        db.execute(text("EXEC usp_CleanAndValidateFlightData"))
        db.commit()

        # Get processing summary
        results["processing_summary"] = processor.get_processing_summary()

        print("‚úÖ Ho√†n th√†nh qu√° tr√¨nh l√†m s·∫°ch v√† x·ª≠ l√Ω d·ªØ li·ªáu")

    except Exception as sp_error:
        print(f"‚ö†Ô∏è L·ªói khi ch·∫°y stored procedures: {sp_error}")
        results["errors"].append(f"L·ªói stored procedure: {str(sp_error)}")
```

#### 5.1. Stored Procedure 1: usp_CleanAndProcessFlightData

**Purpose:** Clean v√† enrich d·ªØ li·ªáu flight

```sql
CREATE PROCEDURE usp_CleanAndProcessFlightData
AS
BEGIN
    -- 1. Clean flight_raw data
    UPDATE flight_raw
    SET 
        route = UPPER(TRIM(route)),
        actype = UPPER(TRIM(actype)),
        flight_date = CONVERT(DATE, flight_date)
    WHERE route IS NOT NULL

    -- 2. Insert into flight_clean_data_stg
    INSERT INTO flight_clean_data_stg (
        route, actype, flight_date, pax, flight_number,
        departure_time, arrival_time, source_file
    )
    SELECT 
        fr.route,
        fr.actype,
        fr.flight_date,
        fr.pax,
        fr.flight_number,
        fr.departure_time,
        fr.arrival_time,
        fr.source_file
    FROM flight_raw fr

    -- 3. Enrich with dimension data
    UPDATE fcd
    SET 
        fcd.route_id = r.Route_ID,
        fcd.distance_km = r.[DISTANCE KM],
        fcd.block_hour = r.[BLOCK HOUR],
        fcd.seat = acs.seat
    FROM flight_clean_data_stg fcd
    LEFT JOIN Route r ON fcd.route = r.ROUTE
    LEFT JOIN actype_seat acs ON fcd.actype = acs.actype
END
```

**C√°c b∆∞·ªõc:**

1. **Clean data**: Uppercase, trim, format dates
2. **Insert v√†o staging**: Copy t·ª´ `flight_raw` ‚Üí `flight_clean_data_stg`
3. **Enrich data**: Join v·ªõi dimension tables
   - `Route`: L·∫•y route_id, distance, block_hour
   - `actype_seat`: L·∫•y s·ªë gh·∫ø

---

#### 5.2. Stored Procedure 2: usp_CleanAndValidateFlightData

**Purpose:** Validate v√† move error data

```sql
CREATE PROCEDURE usp_CleanAndValidateFlightData
AS
BEGIN
    -- 1. Find records with missing dimensions
    INSERT INTO error_table (
        route, actype, flight_date, pax, error_reason, source_file
    )
    SELECT 
        route, actype, flight_date, pax,
        CASE 
            WHEN route_id IS NULL THEN 'Missing Route'
            WHEN seat IS NULL THEN 'Missing Actype'
            ELSE 'Unknown Error'
        END as error_reason,
        source_file
    FROM flight_clean_data_stg
    WHERE route_id IS NULL OR seat IS NULL

    -- 2. Log missing dimensions
    INSERT INTO Missing_Dimensions_Log (Type, Value, Count)
    SELECT 'ROUTE', route, COUNT(*)
    FROM flight_clean_data_stg
    WHERE route_id IS NULL
    GROUP BY route

    INSERT INTO Missing_Dimensions_Log (Type, Value, Count)
    SELECT 'ACTYPE', actype, COUNT(*)
    FROM flight_clean_data_stg
    WHERE seat IS NULL
    GROUP BY actype

    -- 3. Move valid records to final table
    INSERT INTO flight_data_chot (
        route, actype, flight_date, pax, route_id, 
        distance_km, block_hour, seat, source_file
    )
    SELECT 
        route, actype, flight_date, pax, route_id,
        distance_km, block_hour, seat, source_file
    FROM flight_clean_data_stg
    WHERE route_id IS NOT NULL AND seat IS NOT NULL

    -- 4. Clear staging table
    DELETE FROM flight_clean_data_stg
END
```

**C√°c b∆∞·ªõc:**

1. **Identify errors**: Records c√≥ missing dimensions
2. **Move to error_table**: L∆∞u records l·ªói v·ªõi error_reason
3. **Log missing dimensions**: Track actypes/routes thi·∫øu
4. **Move valid records**: Insert v√†o final table `flight_data_chot`
5. **Clear staging**: X√≥a staging table

---

#### 5.3. L·∫•y Processing Summary

**Method:** `processor.get_processing_summary()`

```python
def get_processing_summary(self) -> dict:
    """
    L·∫•y th·ªëng k√™ qu√° tr√¨nh x·ª≠ l√Ω
    """
    from sqlalchemy import func
    from backend.models.flight_raw import FlightRaw
    from backend.models.error_table import ErrorTable
    from backend.models.missing_dimensions_log import MissingDimensionsLog
    from backend.models.flight_data_chot import FlightDataChot
    from backend.models.import_log import ImportLog
    
    # Count records in each table
    raw_count = self.db.query(func.count(FlightRaw.id)).scalar()
    processed_count = self.db.query(func.count(FlightDataChot.id)).scalar()
    error_count = self.db.query(func.count(ErrorTable.id)).scalar()
    
    # Count missing dimensions
    missing_actypes = self.db.query(MissingDimensionsLog).filter(
        MissingDimensionsLog.type == 'ACTYPE'
    ).count()
    
    missing_routes = self.db.query(MissingDimensionsLog).filter(
        MissingDimensionsLog.type == 'ROUTE'
    ).count()
    
    # Count imported files
    imported_files = self.db.query(func.count(ImportLog.id)).scalar()
    
    return {
        "raw_records": raw_count,
        "processed_records": processed_count,
        "error_records": error_count,
        "missing_actypes": missing_actypes,
        "missing_routes": missing_routes,
        "imported_files": imported_files
    }
```

**Returns:**

- `raw_records`: T·ªïng b·∫£n ghi trong `flight_raw`
- `processed_records`: B·∫£n ghi ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng trong `flight_data_chot`
- `error_records`: B·∫£n ghi l·ªói trong `error_table`
- `missing_actypes`: S·ªë l∆∞·ª£ng aircraft types thi·∫øu
- `missing_routes`: S·ªë l∆∞·ª£ng routes thi·∫øu
- `imported_files`: T·ªïng s·ªë files ƒë√£ import

---

### B∆∞·ªõc 6: Tr·∫£ v·ªÅ K·∫øt qu·∫£

```python
success_message = f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {results['processed_files']} file v·ªõi t·ªïng {results['total_rows']} b·∫£n ghi"
if results["skipped_files"] > 0:
    success_message += f" (b·ªè qua {results['skipped_files']} file ƒë√£ import)"

print(f"üéâ {success_message}")

return {"success": True, "message": success_message, **results}
```

**Response Structure:**

```json
{
    "success": true,
    "message": "ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng 5 file v·ªõi t·ªïng 1500 b·∫£n ghi (b·ªè qua 2 file ƒë√£ import)",
    "processed_files": 5,
    "total_rows": 1500,
    "skipped_files": 2,
    "errors": [],
    "file_details": [
        {
            "file_name": "MN_Jan_2024.xlsx",
            "file_type": "MN",
            "rows": 300
        },
        {
            "file_name": "MB_Jan_2024.xlsx",
            "file_type": "MB",
            "rows": 400
        },
        ...
    ],
    "processing_summary": {
        "raw_records": 1500,
        "processed_records": 1450,
        "error_records": 50,
        "missing_actypes": 3,
        "missing_routes": 5,
        "imported_files": 7
    }
}
```

---

## üìä DATABASE SCHEMA

### Tables Involved

#### 1. flight_raw

**Purpose:** L∆∞u d·ªØ li·ªáu raw t·ª´ Excel files

```sql
CREATE TABLE flight_raw (
    id INT IDENTITY(1,1) PRIMARY KEY,
    route VARCHAR(50),
    actype VARCHAR(20),
    flight_date DATE,
    pax INT,
    flight_number VARCHAR(20),
    departure_time TIME,
    arrival_time TIME,
    source_file VARCHAR(255),
    import_date DATETIME,
    created_at DATETIME DEFAULT GETDATE()
)
```

#### 2. import_log

**Purpose:** Track files ƒë√£ import (prevent duplicates)

```sql
CREATE TABLE import_log (
    id INT IDENTITY(1,1) PRIMARY KEY,
    file_name VARCHAR(255) UNIQUE,
    file_type VARCHAR(10),  -- MN, MB, MT
    row_count INT,
    import_date DATETIME,
    status VARCHAR(20),  -- success, failed
    created_at DATETIME DEFAULT GETDATE()
)
```

#### 3. flight_clean_data_stg

**Purpose:** Staging table cho data cleaning

```sql
CREATE TABLE flight_clean_data_stg (
    id INT IDENTITY(1,1) PRIMARY KEY,
    route VARCHAR(50),
    actype VARCHAR(20),
    flight_date DATE,
    pax INT,
    flight_number VARCHAR(20),
    departure_time TIME,
    arrival_time TIME,
    route_id INT,
    distance_km DECIMAL(10,2),
    block_hour DECIMAL(5,2),
    seat INT,
    source_file VARCHAR(255)
)
```

#### 4. flight_data_chot

**Purpose:** Final table ch·ª©a d·ªØ li·ªáu ƒë√£ validated

```sql
CREATE TABLE flight_data_chot (
    id INT IDENTITY(1,1) PRIMARY KEY,
    route VARCHAR(50),
    actype VARCHAR(20),
    flight_date DATE,
    pax INT,
    flight_number VARCHAR(20),
    route_id INT,
    distance_km DECIMAL(10,2),
    block_hour DECIMAL(5,2),
    seat INT,
    load_factor DECIMAL(5,2),  -- Calculated: pax/seat
    source_file VARCHAR(255),
    created_at DATETIME DEFAULT GETDATE()
)
```

#### 5. error_table

**Purpose:** L∆∞u records c√≥ l·ªói

```sql
CREATE TABLE error_table (
    id INT IDENTITY(1,1) PRIMARY KEY,
    route VARCHAR(50),
    actype VARCHAR(20),
    flight_date DATE,
    pax INT,
    error_reason VARCHAR(255),
    source_file VARCHAR(255),
    created_at DATETIME DEFAULT GETDATE()
)
```

#### 6. Missing_Dimensions_Log

**Purpose:** Log dimensions thi·∫øu (actypes/routes)

```sql
CREATE TABLE Missing_Dimensions_Log (
    id INT IDENTITY(1,1) PRIMARY KEY,
    Type VARCHAR(20),  -- ACTYPE, ROUTE
    Value VARCHAR(100),
    Count INT,
    created_at DATETIME DEFAULT GETDATE()
)
```

---

## üîÑ DATA FLOW DIAGRAM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Excel File ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  flight_raw     ‚îÇ  ‚Üê Raw data import
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì  (SP: usp_CleanAndProcessFlightData)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ flight_clean_data_stg  ‚îÇ  ‚Üê Staging + Enrichment
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì  (SP: usp_CleanAndValidateFlightData)
       ‚îÇ
    ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                 ‚îÇ
    ‚Üì                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇflight_data   ‚îÇ  ‚îÇerror_table  ‚îÇ
‚îÇ   _chot      ‚îÇ  ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇMissing_Dimensions_Log   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ KEY FEATURES & OPTIMIZATION

### 1. Duplicate Prevention

**Implementation:**

- Check `import_log` table tr∆∞·ªõc khi process
- Skip files ƒë√£ import
- Return info v·ªÅ skipped files

**Benefits:**

- Tr√°nh duplicate data
- Save processing time
- Audit trail ƒë·∫ßy ƒë·ªß

---

### 2. File Type Detection

**Implementation:**

```python
def find_matching_key(self, filename: str) -> Optional[str]:
    patterns = {
        'MN': ['mn', 'mi·ªÅn nam', 'mien nam'],
        'MB': ['mb', 'mi·ªÅn b·∫Øc', 'mien bac'],
        'MT': ['mt', 'mi·ªÅn trung', 'mien trung']
    }
    
    filename_lower = filename.lower()
    for key, keywords in patterns.items():
        if any(keyword in filename_lower for keyword in keywords):
            return key
    return None
```

**Benefits:**

- T·ª± ƒë·ªông ph√¢n lo·∫°i file
- √Åp d·ª•ng logic x·ª≠ l√Ω ph√π h·ª£p
- Support multiple naming conventions

---

### 3. Transaction Management

**Strategy:**

1. **Transaction 1: Raw Import**
   - Save all files v√†o `flight_raw`
   - Mark imported trong `import_log`
   - Commit

2. **Transaction 2: Data Cleaning**
   - Run stored procedures
   - Move data qua c√°c tables
   - Commit

**Benefits:**

- T√°ch bi·ªát concerns
- N·∫øu cleaning fail, raw data v·∫´n gi·ªØ
- C√≥ th·ªÉ rerun cleaning

---

### 4. Error Handling

**Strategy:**

- Try-catch t·ª´ng file
- Kh√¥ng d·ª´ng batch n·∫øu 1 file fail
- Track errors trong array
- Return t·∫•t c·∫£ errors v·ªÅ frontend

**Example:**

```python
for file in files:
    try:
        # Process file
        ...
    except Exception as e:
        results["errors"].append(f"L·ªói file {filename}: {str(e)}")
        continue  # Continue with next file
```

**Benefits:**

- Robust processing
- Partial success possible
- Full error visibility

---

### 5. Temp Directory Usage

**Implementation:**

```python
with tempfile.TemporaryDirectory() as temp_dir:
    # Save and process files
    ...
# Auto cleanup here
```

**Benefits:**

- Automatic cleanup
- No disk space leak
- Secure (OS handles security)
- Cross-platform compatible

---

### 6. Processing Summary

**Real-time Statistics:**

- `raw_records`: Input data count
- `processed_records`: Successfully processed
- `error_records`: Failed validations
- `missing_actypes`: Dimensions thi·∫øu
- `missing_routes`: Routes thi·∫øu
- `imported_files`: Total files imported

**Benefits:**

- Transparency
- Data quality monitoring
- Identify issues quickly

---

## üö® ERROR SCENARIOS & HANDLING

### Scenario 1: Invalid File Extension

**Error:** User upload `.csv` ho·∫∑c `.pdf`

**Handling:**

```python
if not file.filename.lower().endswith((".xlsx", ".xls")):
    results["errors"].append(
        f"File {file.filename} kh√¥ng ph·∫£i l√† Excel file"
    )
    continue
```

**Result:** Skip file, th√¥ng b√°o error, continue v·ªõi files kh√°c

---

### Scenario 2: File Already Imported

**Error:** Upload file ƒë√£ import tr∆∞·ªõc ƒë√≥

**Handling:**

```python
if processor.is_file_imported(filename_only):
    print(f"‚è≠Ô∏è File {filename_only} ƒë√£ ƒë∆∞·ª£c import tr∆∞·ªõc ƒë√≥")
    results["skipped_files"] += 1
    continue
```

**Result:** Skip file, increment `skipped_files`, kh√¥ng b√°o error

---

### Scenario 3: Unknown File Type

**Error:** Filename kh√¥ng match patterns (MN/MB/MT)

**Handling:**

```python
file_type = processor.find_matching_key(filename_only)
if not file_type:
    results["errors"].append(
        f"Kh√¥ng th·ªÉ x√°c ƒë·ªãnh lo·∫°i file: {filename_only}"
    )
    continue
```

**Result:** Skip file, th√¥ng b√°o error

---

### Scenario 4: Empty Excel File

**Error:** Excel file kh√¥ng c√≥ data ho·∫∑c empty DataFrame

**Handling:**

```python
df = processor.process_excel_file(file_path, filename_only)

if df.empty:
    results["errors"].append(
        f"Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ file {filename_only}"
    )
    continue
```

**Result:** Skip file, th√¥ng b√°o error

---

### Scenario 5: Missing Dimensions

**Error:** Data c√≥ actype ho·∫∑c route kh√¥ng t·ªìn t·∫°i trong dimension tables

**Handling:**

1. Stored procedure detect missing dimensions
2. Move records v√†o `error_table`
3. Log v√†o `Missing_Dimensions_Log`
4. Return summary v·ªõi `missing_actypes` v√† `missing_routes`

**Result:**

- Error records ƒë∆∞·ª£c track
- User bi·∫øt c·∫ßn b·ªï sung dimensions n√†o
- C√≥ th·ªÉ export missing dimensions ƒë·ªÉ ƒëi·ªÅn th√¥ng tin

---

### Scenario 6: Database Error

**Error:** Connection lost, constraint violation, etc.

**Handling:**

```python
try:
    # Process all files
    ...
except Exception as e:
    db.rollback()
    error_msg = f"L·ªói upload files: {str(e)}"
    logging.error(error_msg)
    raise HTTPException(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        detail=error_msg
    )
```

**Result:**

- Rollback transaction
- Log error
- Return HTTP 500 error
- Frontend hi·ªÉn th·ªã error toast

---

## üìù BEST PRACTICES

### 1. Logging Strategy

```python
# Console logging for debugging
print(f"üì§ B·∫Øt ƒë·∫ßu upload v√† x·ª≠ l√Ω {len(files)} file Excel...")
print(f"üìÅ Processing file: {file.filename} -> {filename_only}")
print(f"‚úÖ ƒê√£ l∆∞u {row_count} b·∫£n ghi t·ª´ file {filename_only}")

# Error logging
logging.error(f"Error processing Excel data: {e}")
```

**Benefits:**

- Easy debugging
- Production monitoring
- Audit trail

---

### 2. Validation Layers

**Layer 1: File Extension**

```python
if not file.filename.lower().endswith((".xlsx", ".xls")):
    # Reject
```

**Layer 2: Duplicate Check**

```python
if processor.is_file_imported(filename_only):
    # Skip
```

**Layer 3: File Type Detection**

```python
file_type = processor.find_matching_key(filename_only)
if not file_type:
    # Reject
```

**Layer 4: Data Validation**

```python
if df.empty:
    # Reject
```

**Layer 5: Business Rules (Stored Procedures)**

- Check dimensions exist
- Validate data integrity
- Move errors to separate table

---

### 3. Response Structure

```python
return {
    "success": True,
    "message": "...",
    "processed_files": 5,
    "total_rows": 1500,
    "skipped_files": 2,
    "errors": [...],
    "file_details": [...],
    "processing_summary": {...}
}
```

**Benefits:**

- Complete information
- Easy frontend parsing
- Consistent structure

---

### 4. Frontend Error Display

```typescript
// Show first 3 errors with delay
result.errors.slice(0, 3).forEach((error: string, index: number) => {
    setTimeout(() => {
        toast.error(`L·ªói ${index + 1}`, {
            description: error,
        })
    }, (index + 1) * 1000)
})

// Show "more errors" message
if (result.errors.length > 3) {
    setTimeout(() => {
        toast.info("C√≥ th√™m l·ªói kh√°c", {
            description: `V√† ${result.errors.length - 3} l·ªói kh√°c...`,
        })
    }, 4000)
}
```

**Benefits:**

- Don't overwhelm user
- Sequential display (readable)
- Complete error info in console

---

## üîê SECURITY CONSIDERATIONS

### 1. File Validation

- Check extension before processing
- Validate file size (prevent DOS)
- Scan for malicious content (if needed)

### 2. SQL Injection Prevention

- Use parameterized queries
- SQLAlchemy ORM (automatic escaping)
- Stored procedures (controlled execution)

### 3. Temp File Security

- Use OS-provided temp directory
- Automatic cleanup
- Unique names (no collision)

### 4. Database Security

- Use connection pooling
- Proper transaction isolation
- Rollback on errors

---

## üéØ PERFORMANCE OPTIMIZATION

### 1. Bulk Insert

```python
# Bad: Insert one by one with commit
for record in records:
    db.add(FlightRaw(**record))
    db.commit()  # ‚ùå Slow

# Good: Bulk insert with single commit
for record in records:
    db.add(FlightRaw(**record))
db.commit()  # ‚úÖ Fast
```

### 2. Temp Directory

- Faster than persistent storage
- Often in memory (tmpfs on Linux)
- OS-optimized

### 3. Stored Procedures

- Compiled execution plan
- Reduce network round-trips
- Batch operations
- Database-side processing

### 4. Async File Reading

```python
content = await file.read()  # Non-blocking I/O
```

---

## üìö FUTURE ENHANCEMENTS

### 1. Parallel Processing

```python
# Use asyncio for parallel file processing
import asyncio

async def process_file_async(file, processor, db):
    # Process file logic
    ...

# Process multiple files in parallel
tasks = [process_file_async(file, processor, db) for file in files]
await asyncio.gather(*tasks)
```

### 2. Progress Tracking

```python
# WebSocket or SSE for real-time progress
async def upload_with_progress(files):
    for i, file in enumerate(files):
        # Process file
        ...
        # Send progress
        await websocket.send({
            "progress": (i + 1) / len(files) * 100,
            "current_file": file.filename
        })
```

### 3. Background Jobs

```python
# Use Celery or RQ for background processing
from celery import Celery

@app.task
def process_excel_background(file_paths):
    # Long-running process
    ...
    
# Immediate response to user
task = process_excel_background.delay(file_paths)
return {"task_id": task.id}
```

### 4. File Validation Service

```python
# Dedicated service for file validation
class FileValidator:
    def validate_schema(self, df, expected_columns):
        # Check columns match
        ...
    
    def validate_data_types(self, df, schema):
        # Check data types
        ...
    
    def validate_business_rules(self, df):
        # Check business constraints
        ...
```

---

## üèÅ CONCLUSION

Flow x·ª≠ l√Ω batch file Excel n√†y implement m·ªôt pipeline ho√†n ch·ªânh:

1. ‚úÖ **Upload**: Multi-file upload v·ªõi validation
2. ‚úÖ **Processing**: Ph√¢n lo·∫°i, clean, transform data
3. ‚úÖ **Storage**: L∆∞u v√†o database theo layers
4. ‚úÖ **Validation**: Multiple validation layers
5. ‚úÖ **Error Handling**: Robust error tracking
6. ‚úÖ **Feedback**: Real-time progress v√† results

**Key Strengths:**

- Robust error handling
- Transaction safety
- Duplicate prevention
- Detailed tracking & logging
- User-friendly feedback
- Scalable architecture

**Technologies Used:**

- **Frontend**: React + TypeScript + Sonner (toast)
- **Backend**: FastAPI + SQLAlchemy + Pandas
- **Database**: SQL Server + Stored Procedures
- **File Handling**: Python tempfile module

---

## üìñ REFERENCES

- [FastAPI File Upload](https://fastapi.tiangolo.com/tutorial/request-files/)
- [Pandas Excel Operations](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html)
- [SQLAlchemy ORM](https://docs.sqlalchemy.org/en/14/orm/)
- [Python tempfile Module](https://docs.python.org/3/library/tempfile.html)

---

**Document Version:** 1.0  
**Last Updated:** 2024-01-13  
**Author:** AI Assistant  
**Project:** Airline Data Processing System
